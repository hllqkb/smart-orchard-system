"""
æ™ºèƒ½æœå›­æ£€æµ‹ç³»ç»Ÿ - æ¨¡å‹è®­ç»ƒæ¨¡å—
"""

import streamlit as st
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision.models import resnet18, resnet50, vgg16, efficientnet_b0
import numpy as np
import pandas as pd
from PIL import Image
import os
import json
import zipfile
import tempfile
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

from config.settings import DATA_DIR, MODELS_DIR
from config.database import db_manager

# å¯¼å…¥æ·±åº¦å­¦ä¹ ç½‘ç»œæ¶æ„
class LeNet(nn.Module):
    """LeNet-5 ç½‘ç»œæ¶æ„"""
    def __init__(self, num_classes=10):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)
        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)
        
    def forward(self, x):
        x = self.pool1(torch.relu(self.conv1(x)))
        x = self.pool2(torch.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class AlexNet(nn.Module):
    """AlexNet ç½‘ç»œæ¶æ„"""
    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

class SimpleRCNN(nn.Module):
    """ç®€åŒ–çš„R-CNNæ¶æ„ç”¨äºç›®æ ‡æ£€æµ‹"""
    def __init__(self, num_classes=2, backbone='resnet18'):
        super(SimpleRCNN, self).__init__()
        if backbone == 'resnet18':
            self.backbone = resnet18(pretrained=True)
            self.backbone.fc = nn.Identity()
            feature_dim = 512
        elif backbone == 'resnet50':
            self.backbone = resnet50(pretrained=True)
            self.backbone.fc = nn.Identity()
            feature_dim = 2048
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
        
        # å›å½’å¤´ï¼ˆè¾¹ç•Œæ¡†ï¼‰
        self.bbox_regressor = nn.Sequential(
            nn.Linear(feature_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 4)  # x, y, w, h
        )
    
    def forward(self, x):
        features = self.backbone(x)
        cls_scores = self.classifier(features)
        bbox_pred = self.bbox_regressor(features)
        return cls_scores, bbox_pred

class CustomDataset(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç±»"""
    def __init__(self, data_dir, transform=None, task_type='classification'):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.task_type = task_type
        self.samples = []
        self.classes = []
        
        if task_type == 'classification':
            self._load_classification_data()
        elif task_type == 'detection':
            self._load_detection_data()
    
    def _load_classification_data(self):
        """åŠ è½½åˆ†ç±»æ•°æ®"""
        for class_dir in self.data_dir.iterdir():
            if class_dir.is_dir():
                class_name = class_dir.name
                if class_name not in self.classes:
                    self.classes.append(class_name)
                
                class_idx = self.classes.index(class_name)
                
                for img_file in class_dir.glob('*.jpg'):
                    self.samples.append((str(img_file), class_idx))
    
    def _load_detection_data(self):
        """åŠ è½½æ£€æµ‹æ•°æ®"""
        # å‡è®¾æœ‰annotations.jsonæ–‡ä»¶åŒ…å«æ ‡æ³¨ä¿¡æ¯
        annotations_file = self.data_dir / 'annotations.json'
        if annotations_file.exists():
            with open(annotations_file, 'r') as f:
                annotations = json.load(f)
            
            for ann in annotations:
                img_path = self.data_dir / ann['image']
                if img_path.exists():
                    self.samples.append((str(img_path), ann))
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        if self.task_type == 'classification':
            img_path, label = self.samples[idx]
            image = Image.open(img_path).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            return image, label
        
        elif self.task_type == 'detection':
            img_path, annotation = self.samples[idx]
            image = Image.open(img_path).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            # æå–è¾¹ç•Œæ¡†å’Œç±»åˆ«
            bbox = annotation.get('bbox', [0, 0, 1, 1])
            class_id = annotation.get('class_id', 0)
            
            return image, torch.tensor(bbox, dtype=torch.float32), torch.tensor(class_id, dtype=torch.long)

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.models_dir = MODELS_DIR / "trained_models"
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # å¯ç”¨çš„ç½‘ç»œæ¶æ„
        self.available_architectures = {
            'LeNet': LeNet,
            'AlexNet': AlexNet,
            'ResNet18': lambda num_classes: self._get_pretrained_resnet(18, num_classes),
            'ResNet50': lambda num_classes: self._get_pretrained_resnet(50, num_classes),
            'VGG16': lambda num_classes: self._get_pretrained_vgg(num_classes),
            'EfficientNet-B0': lambda num_classes: self._get_pretrained_efficientnet(num_classes),
            'Simple R-CNN': SimpleRCNN
        }
    
    def _get_pretrained_resnet(self, layers, num_classes):
        """è·å–é¢„è®­ç»ƒçš„ResNetæ¨¡å‹"""
        if layers == 18:
            model = resnet18(pretrained=True)
        elif layers == 50:
            model = resnet50(pretrained=True)
        
        # ä¿®æ”¹æœ€åä¸€å±‚
        model.fc = nn.Linear(model.fc.in_features, num_classes)
        return model
    
    def _get_pretrained_vgg(self, num_classes):
        """è·å–é¢„è®­ç»ƒçš„VGGæ¨¡å‹"""
        model = vgg16(pretrained=True)
        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)
        return model
    
    def _get_pretrained_efficientnet(self, num_classes):
        """è·å–é¢„è®­ç»ƒçš„EfficientNetæ¨¡å‹"""
        model = efficientnet_b0(pretrained=True)
        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
        return model
    
    def prepare_data(self, data_path, task_type='classification', batch_size=32, val_split=0.2):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # æ•°æ®å¢å¼º
        if task_type == 'classification':
            train_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(10),
                transforms.ColorJitter(brightness=0.2, contrast=0.2),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            val_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            train_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            val_transform = train_transform
        
        # åˆ›å»ºæ•°æ®é›†
        full_dataset = CustomDataset(data_path, transform=train_transform, task_type=task_type)
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        dataset_size = len(full_dataset)
        val_size = int(dataset_size * val_split)
        train_size = dataset_size - val_size
        
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size]
        )
        
        # ä¸ºéªŒè¯é›†è®¾ç½®ä¸åŒçš„å˜æ¢
        val_dataset.dataset.transform = val_transform
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        
        return train_loader, val_loader, full_dataset.classes
    
    def create_model(self, architecture, num_classes, task_type='classification'):
        """åˆ›å»ºæ¨¡å‹"""
        if architecture in self.available_architectures:
            if architecture == 'Simple R-CNN' and task_type == 'detection':
                model = self.available_architectures[architecture](num_classes)
            else:
                model = self.available_architectures[architecture](num_classes)
            
            model = model.to(self.device)
            return model
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¶æ„: {architecture}")
    
    def train_model(self, model, train_loader, val_loader, num_epochs=10, learning_rate=0.001, task_type='classification'):
        """è®­ç»ƒæ¨¡å‹"""
        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        if task_type == 'classification':
            criterion = nn.CrossEntropyLoss()
        elif task_type == 'detection':
            cls_criterion = nn.CrossEntropyLoss()
            bbox_criterion = nn.MSELoss()
        
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        train_accs = []
        val_accs = []
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            running_loss = 0.0
            correct = 0
            total = 0
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, batch in enumerate(train_loader):
                if task_type == 'classification':
                    inputs, labels = batch
                    inputs, labels = inputs.to(self.device), labels.to(self.device)
                    
                    optimizer.zero_grad()
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()
                    
                    running_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
                
                elif task_type == 'detection':
                    inputs, bboxes, labels = batch
                    inputs = inputs.to(self.device)
                    bboxes = bboxes.to(self.device)
                    labels = labels.to(self.device)
                    
                    optimizer.zero_grad()
                    cls_outputs, bbox_outputs = model(inputs)
                    
                    cls_loss = cls_criterion(cls_outputs, labels)
                    bbox_loss = bbox_criterion(bbox_outputs, bboxes)
                    loss = cls_loss + bbox_loss
                    
                    loss.backward()
                    optimizer.step()
                    
                    running_loss += loss.item()
                    _, predicted = torch.max(cls_outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
                
                # æ›´æ–°è¿›åº¦
                progress = (i + 1) / len(train_loader)
                progress_bar.progress(progress)
                status_text.text(f"Epoch {epoch+1}/{num_epochs} - Batch {i+1}/{len(train_loader)}")
            
            train_loss = running_loss / len(train_loader)
            train_acc = 100 * correct / total
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    if task_type == 'classification':
                        inputs, labels = batch
                        inputs, labels = inputs.to(self.device), labels.to(self.device)
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)
                        
                        val_loss += loss.item()
                        _, predicted = torch.max(outputs.data, 1)
                        val_total += labels.size(0)
                        val_correct += (predicted == labels).sum().item()
                    
                    elif task_type == 'detection':
                        inputs, bboxes, labels = batch
                        inputs = inputs.to(self.device)
                        bboxes = bboxes.to(self.device)
                        labels = labels.to(self.device)
                        
                        cls_outputs, bbox_outputs = model(inputs)
                        cls_loss = cls_criterion(cls_outputs, labels)
                        bbox_loss = bbox_criterion(bbox_outputs, bboxes)
                        loss = cls_loss + bbox_loss
                        
                        val_loss += loss.item()
                        _, predicted = torch.max(cls_outputs.data, 1)
                        val_total += labels.size(0)
                        val_correct += (predicted == labels).sum().item()
            
            val_loss = val_loss / len(val_loader)
            val_acc = 100 * val_correct / val_total
            
            # è®°å½•å†å²
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_accs.append(train_acc)
            val_accs.append(val_acc)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            # æ˜¾ç¤ºè¿›åº¦
            st.write(f"Epoch {epoch+1}/{num_epochs}")
            st.write(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            st.write(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            st.write("---")
        
        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accs': train_accs,
            'val_accs': val_accs
        }
    
    def save_model(self, model, model_name, classes, training_history, task_type='classification'):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir = self.models_dir / f"{model_name}_{timestamp}"
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        model_path = model_dir / "model.pth"
        torch.save(model.state_dict(), model_path)
        
        # ä¿å­˜æ¨¡å‹ä¿¡æ¯
        model_info = {
            'model_name': model_name,
            'num_classes': len(classes),
            'classes': classes,
            'task_type': task_type,
            'training_history': training_history,
            'timestamp': timestamp,
            'device': str(self.device)
        }
        
        info_path = model_dir / "model_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        return str(model_dir)
    
    def plot_training_history(self, history):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(history['train_losses'], label='Train Loss')
        ax1.plot(history['val_losses'], label='Val Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(history['train_accs'], label='Train Acc')
        ax2.plot(history['val_accs'], label='Val Acc')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        return fig

    def show_training_interface(self, user_id):
        """æ˜¾ç¤ºè®­ç»ƒç•Œé¢"""
        st.markdown('<h1 class="main-header">ğŸ¤– æ¨¡å‹è®­ç»ƒä¸­å¿ƒ</h1>', unsafe_allow_html=True)

        st.markdown("""
        <div class="info-box">
            <h3>ğŸ¯ æ™ºèƒ½æ¨¡å‹è®­ç»ƒ</h3>
            <p>ä¸Šä¼ æ‚¨çš„æ•°æ®é›†ï¼Œé€‰æ‹©ç½‘ç»œæ¶æ„ï¼Œè®­ç»ƒä¸“å±çš„AIæ¨¡å‹</p>
        </div>
        """, unsafe_allow_html=True)

        # è®­ç»ƒé…ç½®
        col1, col2 = st.columns([2, 1])

        with col1:
            # ä»»åŠ¡ç±»å‹é€‰æ‹©
            task_type = st.selectbox(
                "é€‰æ‹©ä»»åŠ¡ç±»å‹",
                ["classification", "detection"],
                format_func=lambda x: "å›¾åƒåˆ†ç±»" if x == "classification" else "ç›®æ ‡æ£€æµ‹"
            )

            # ç½‘ç»œæ¶æ„é€‰æ‹©
            if task_type == "classification":
                available_archs = [k for k in self.available_architectures.keys() if k != 'Simple R-CNN']
            else:
                available_archs = ['Simple R-CNN', 'ResNet18', 'ResNet50']

            architecture = st.selectbox("é€‰æ‹©ç½‘ç»œæ¶æ„", available_archs)

            # æ˜¾ç¤ºæ¶æ„ä¿¡æ¯
            arch_info = {
                'LeNet': "ç»å…¸çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œé€‚åˆç®€å•çš„å›¾åƒåˆ†ç±»ä»»åŠ¡",
                'AlexNet': "æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„å…ˆé©±ï¼Œé€‚åˆå¤æ‚å›¾åƒåˆ†ç±»",
                'ResNet18': "18å±‚æ®‹å·®ç½‘ç»œï¼Œå¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦",
                'ResNet50': "50å±‚æ®‹å·®ç½‘ç»œï¼Œæ›´å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›",
                'VGG16': "16å±‚VGGç½‘ç»œï¼Œç‰¹å¾æå–èƒ½åŠ›å¼º",
                'EfficientNet-B0': "é«˜æ•ˆç½‘ç»œï¼Œåœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡é—´å–å¾—å¹³è¡¡",
                'Simple R-CNN': "ç®€åŒ–çš„R-CNNæ¶æ„ï¼Œé€‚åˆç›®æ ‡æ£€æµ‹ä»»åŠ¡"
            }

            st.info(f"ğŸ“‹ {arch_info.get(architecture, 'è‡ªå®šä¹‰ç½‘ç»œæ¶æ„')}")

        with col2:
            # è®­ç»ƒå‚æ•°
            st.markdown("### âš™ï¸ è®­ç»ƒå‚æ•°")
            num_epochs = st.slider("è®­ç»ƒè½®æ•°", 1, 100, 10)
            batch_size = st.selectbox("æ‰¹æ¬¡å¤§å°", [8, 16, 32, 64], index=2)
            learning_rate = st.selectbox("å­¦ä¹ ç‡", [0.0001, 0.001, 0.01, 0.1], index=1)
            val_split = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.1, 0.5, 0.2)

        # æ•°æ®ä¸Šä¼ 
        st.markdown("## ğŸ“ æ•°æ®é›†ä¸Šä¼ ")

        if task_type == "classification":
            st.markdown("""
            **åˆ†ç±»æ•°æ®é›†æ ¼å¼è¦æ±‚ï¼š**
            - ä¸Šä¼ ZIPæ–‡ä»¶
            - æ–‡ä»¶å¤¹ç»“æ„ï¼š`ç±»åˆ«å/å›¾åƒæ–‡ä»¶.jpg`
            - æ”¯æŒæ ¼å¼ï¼šJPG, PNG
            """)
        else:
            st.markdown("""
            **æ£€æµ‹æ•°æ®é›†æ ¼å¼è¦æ±‚ï¼š**
            - ä¸Šä¼ ZIPæ–‡ä»¶
            - åŒ…å«å›¾åƒæ–‡ä»¶å’Œannotations.jsonæ ‡æ³¨æ–‡ä»¶
            - æ ‡æ³¨æ ¼å¼ï¼š`{"image": "æ–‡ä»¶å", "bbox": [x, y, w, h], "class_id": 0}`
            """)

        uploaded_file = st.file_uploader(
            "é€‰æ‹©æ•°æ®é›†ZIPæ–‡ä»¶",
            type=['zip'],
            help="ä¸Šä¼ åŒ…å«è®­ç»ƒæ•°æ®çš„ZIPæ–‡ä»¶"
        )

        if uploaded_file is not None:
            # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
            st.success(f"âœ… å·²ä¸Šä¼ æ•°æ®é›†: {uploaded_file.name} ({uploaded_file.size / (1024*1024):.2f} MB)")

            # å¼€å§‹è®­ç»ƒæŒ‰é’®
            if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", use_container_width=True):
                self._start_training(
                    user_id, uploaded_file, task_type, architecture,
                    num_epochs, batch_size, learning_rate, val_split
                )

        # æ˜¾ç¤ºå·²è®­ç»ƒçš„æ¨¡å‹
        self._show_trained_models(user_id)

    def _start_training(self, user_id, uploaded_file, task_type, architecture,
                       num_epochs, batch_size, learning_rate, val_split):
        """å¼€å§‹è®­ç»ƒè¿‡ç¨‹"""
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)

                # è§£å‹æ•°æ®é›†
                st.info("ğŸ“¦ æ­£åœ¨è§£å‹æ•°æ®é›†...")
                with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:
                    zip_ref.extractall(temp_path)

                # æŸ¥æ‰¾æ•°æ®ç›®å½•
                data_dirs = [d for d in temp_path.iterdir() if d.is_dir()]
                if not data_dirs:
                    st.error("âŒ æ•°æ®é›†æ ¼å¼é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®ç›®å½•")
                    return

                data_path = data_dirs[0]

                # å‡†å¤‡æ•°æ®
                st.info("ğŸ”„ æ­£åœ¨å‡†å¤‡è®­ç»ƒæ•°æ®...")
                train_loader, val_loader, classes = self.prepare_data(
                    data_path, task_type, batch_size, val_split
                )

                st.success(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼š{len(classes)} ä¸ªç±»åˆ«ï¼Œ{len(train_loader.dataset)} è®­ç»ƒæ ·æœ¬ï¼Œ{len(val_loader.dataset)} éªŒè¯æ ·æœ¬")

                # åˆ›å»ºæ¨¡å‹
                st.info(f"ğŸ—ï¸ æ­£åœ¨åˆ›å»º {architecture} æ¨¡å‹...")
                model = self.create_model(architecture, len(classes), task_type)

                # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ€»å‚æ•°é‡", f"{total_params:,}")
                with col2:
                    st.metric("å¯è®­ç»ƒå‚æ•°", f"{trainable_params:,}")
                with col3:
                    st.metric("è®¾å¤‡", str(self.device))

                # å¼€å§‹è®­ç»ƒ
                st.info("ğŸ¯ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
                history = self.train_model(
                    model, train_loader, val_loader,
                    num_epochs, learning_rate, task_type
                )

                # æ˜¾ç¤ºè®­ç»ƒç»“æœ
                st.success("ğŸ‰ è®­ç»ƒå®Œæˆï¼")

                # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
                fig = self.plot_training_history(history)
                st.pyplot(fig)

                # ä¿å­˜æ¨¡å‹
                model_name = f"{architecture}_{task_type}"
                model_path = self.save_model(model, model_name, classes, history, task_type)

                st.success(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

                # è®°å½•è®­ç»ƒæ—¥å¿—
                db_manager.log_action(
                    user_id,
                    "MODEL_TRAINED",
                    f"Trained {architecture} for {task_type} with {len(classes)} classes"
                )

        except Exception as e:
            st.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

    def _show_trained_models(self, user_id):
        """æ˜¾ç¤ºå·²è®­ç»ƒçš„æ¨¡å‹"""
        st.markdown("## ğŸ“š å·²è®­ç»ƒçš„æ¨¡å‹")

        if not self.models_dir.exists():
            st.info("æš‚æ— å·²è®­ç»ƒçš„æ¨¡å‹")
            return

        model_dirs = [d for d in self.models_dir.iterdir() if d.is_dir()]

        if not model_dirs:
            st.info("æš‚æ— å·²è®­ç»ƒçš„æ¨¡å‹")
            return

        # æŒ‰æ—¶é—´æ’åº
        model_dirs.sort(key=lambda x: x.stat().st_mtime, reverse=True)

        for model_dir in model_dirs[:10]:  # æ˜¾ç¤ºæœ€è¿‘10ä¸ªæ¨¡å‹
            info_file = model_dir / "model_info.json"
            if info_file.exists():
                with open(info_file, 'r', encoding='utf-8') as f:
                    model_info = json.load(f)

                with st.expander(f"ğŸ¤– {model_info['model_name']} - {model_info['timestamp']}"):
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.write(f"**ä»»åŠ¡ç±»å‹:** {model_info['task_type']}")
                        st.write(f"**ç±»åˆ«æ•°é‡:** {model_info['num_classes']}")
                        st.write(f"**è®­ç»ƒè®¾å¤‡:** {model_info['device']}")

                    with col2:
                        st.write(f"**ç±»åˆ«åˆ—è¡¨:**")
                        for i, cls in enumerate(model_info['classes']):
                            st.write(f"  {i}: {cls}")

                    with col3:
                        # æ˜¾ç¤ºæœ€ç»ˆæ€§èƒ½
                        history = model_info['training_history']
                        if history['val_accs']:
                            final_acc = history['val_accs'][-1]
                            st.metric("æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡", f"{final_acc:.2f}%")

                        # ä¸‹è½½æŒ‰é’®
                        model_file = model_dir / "model.pth"
                        if model_file.exists():
                            with open(model_file, 'rb') as f:
                                st.download_button(
                                    "ğŸ“¥ ä¸‹è½½æ¨¡å‹",
                                    data=f.read(),
                                    file_name=f"{model_info['model_name']}.pth",
                                    mime="application/octet-stream"
                                )

# å…¨å±€æ¨¡å‹è®­ç»ƒå™¨å®ä¾‹
model_trainer = ModelTrainer()
